{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aZrjb_Zq0pmv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THEORY QUESTIONS:-**"
      ],
      "metadata": {
        "id": "l2Yv2d330skw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRACTICAL QUESTIONS:-**"
      ],
      "metadata": {
        "id": "p4QByyYxAqsM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q:1-Can we use Bagging for regression problems?**\n",
        "**ANS:**\n",
        "Yes, bagging (Bootstrap Aggregating) is commonly used for regression problems to improve model stability and accuracy by reducing variance, typically by training multiple base regressors (like decision trees) on different bootstrapped samples of data and averaging their predictions for a final, more robust output. While it's especially effective for unstable models prone to overfitting (high variance), it can also be applied to stable models to smooth out noise, although benefits might be less dramatic.\n",
        "\n",
        "**How Bagging Works for Regression**\n",
        "1)Bootstrap Samples:-\n",
        "-Multiple random subsets of the original training data are created with replacement (bootstrapping).\n",
        "\n",
        "2)Base Model Training:-\n",
        "-A regression model (e.g., a decision tree regressor) is trained independently on each bootstrap sample.\n",
        "\n",
        "3)Prediction Aggregation:-\n",
        "-For a new input, each base model makes a prediction, and the final output is the average (mean) of all these individual predictions.\n",
        "\n",
        "**Q:2-What is the difference between multiple model training and single model training?**\n",
        "**ANS:**\n",
        "Single model training uses one model for all tasks/data, offering simplicity and consistency but potentially less accuracy for diverse problems, while multiple model training uses specialized models (or ensembles) for specific data segments or tasks, boosting accuracy and handling complexity but increasing resource needs, complexity, and maintenance. The choice depends on the problem's complexity, data heterogeneity, and resource availability; single models suit broad, consistent needs, while multiple models excel in specialized, high-accuracy scenarios.\n",
        "\n",
        "#Single Model Training:-\n",
        "1)Concept: One model learns from all available data and features to perform all tasks.\n",
        "\n",
        "2)Pros: Simpler architecture, easier management, consistent outputs, cost-effective.\n",
        "\n",
        "3)Cons: May not capture nuanced patterns in diverse data, potentially lower accuracy for specific segments.\n",
        "\n",
        "4)Use Case: A general-purpose assistant handling broad tasks like drafting emails and summarizing text.\n",
        "\n",
        "#Multiple Model Training:-\n",
        "1)Concept: Trains separate models for different data subsets, tasks, or combines several models (ensemble).\n",
        "\n",
        "2)Pros: Higher accuracy due to specialization, better performance on complex, heterogeneous data.\n",
        "\n",
        "3)Cons: More complex to build, manage, and integrate; higher computational cost.\n",
        "\n",
        "4)Use Case: A healthcare system with separate models for triage, diagnosis, and treatment recommendations, or an ensemble combining models for final prediction.\n",
        "\n",
        "**Q:3-Explain the concept of feature randomness in Random Forest.**\n",
        "**ANS:**\n",
        "Feature randomness, also known as the random subspace method or feature bagging, is a core component of the Random Forest algorithm that introduces diversity among the individual decision trees in the forest. This diversity is crucial for improving the model's accuracy and robustness, as it helps to reduce correlation among the trees and prevent overfitting.\n",
        "\n",
        "#How Feature Randomness Works\n",
        "In a standard decision tree algorithm, at each potential split, the model considers all available features and selects the one that produces the best split (e.g., maximizing information gain or minimizing Gini impurity).\n",
        "\n",
        "In contrast, the Random Forest algorithm modifies this process by implementing feature randomness at every node:\n",
        "\n",
        "1)Random Subset Selection: Instead of evaluating all features, the algorithm first selects a random subset of the total features.\n",
        "\n",
        "2)Optimal Split within Subset: Only within this limited, random subset does the algorithm search for the best feature and optimal split point.\n",
        "\n",
        "#The Importance of Feature Randomness:-\n",
        "\n",
        "-The primary goal of feature randomness is to ensure that the individual trees are as independent and diverse as possible.\n",
        "\n",
        "1)Reduces Correlation:-\n",
        "If a few strong predictor features dominate the data, a standard bagging approach would result in most trees selecting those features at the top nodes, making the trees highly correlated and limiting the performance gains of the ensemble. By forcing each tree to work with a random subset, the influence of any single strong feature is diminished, leading to a collection of less correlated (more diverse) trees.\n",
        "\n",
        "2)Prevents Overfitting:-\n",
        "The diversity among trees helps to prevent the model from learning highly specific, noisy patterns in the training data, which is a common problem with single, deep decision trees.\n",
        "\n",
        "3)Improves Generalization:-\n",
        "By combining the predictions (via majority voting for classification or averaging for regression) from many diverse, uncorrelated trees, the random forest produces a more stable, accurate, and generalized final prediction.\n",
        "\n",
        "**Q:4-What is OOB (Out-of-Bag) Score?**\n",
        "**ANS:**\n",
        "The OOB (Out-of-Bag) Score is an internal, unbiased estimate of a machine learning model's performance, commonly used in ensemble methods like Random Forests, that leverages data points not used in training individual trees to evaluate the model, eliminating the need for a separate validation set and saving computation time.\n",
        "\n",
        "#How it Works:-\n",
        "1)Bootstrapping: Each decision tree in the forest is trained on a random sample of the original data, with replacement (bootstrapping).\n",
        "\n",
        "2)OOB Samples: For each tree, roughly 37% of the original data points are left out (the \"out-of-bag\" samples).\n",
        "\n",
        "3)Prediction on OOB Data: Each data point is predicted by all the trees that did not see it during their training.\n",
        "\n",
        "4)Aggregated Prediction: For a given data point, the predictions from all relevant trees are combined (e.g., majority vote for classification).\n",
        "\n",
        "5)Score Calculation: The OOB score is the proportion of these predictions that are correct, effectively acting as a test set's accuracy.\n",
        "\n",
        "#Key Benefits:-\n",
        "-No Separate Validation Set Needed: Saves time and data by using the training data itself for evaluation.\n",
        "\n",
        "-Unbiased Estimate: Provides a robust measure of generalization error, similar to cross-validation.\n",
        "\n",
        "-Efficient: Reduces computational overhead by avoiding explicit validation folds.\n",
        "\n",
        "**Q:5-How can you measure the importance of features in a Random Forest model?**\n",
        "**ANS:**\n",
        "You measure Random Forest feature importance primarily through Mean Decrease in Impurity (MDI), which sums how much each feature reduces Gini impurity (or variance in regression) across all trees, and Permutation Importance, which checks performance drop when a feature is shuffled, plus more advanced methods like SHAP values for localized explanations, giving a robust view of feature influence.\n",
        "\n",
        "#PRIMARY METHODS:-\n",
        "1)Mean Decrease in Impurity (MDI) / Gini Importance:\n",
        "\n",
        "-How it works:\n",
        "Calculates the average reduction in impurity (Gini for classification, variance for regression) at each split where a feature is used, summed across all trees.\n",
        "\n",
        "-Benefit:\n",
        "Built-in and computationally fast (often accessible via .feature_importances_ in libraries like scikit-learn).\n",
        "\n",
        "-Limitation:\n",
        "Can be biased towards high-cardinality (many unique values) features.\n",
        "\n",
        "2)Permutation Feature Importance:\n",
        "\n",
        "-How it works:\n",
        "Measures the decrease in model performance (e.g., accuracy, R²) when a single feature's values are randomly shuffled, disrupting its relationship with the target.\n",
        "\n",
        "-Benefit:\n",
        "More reliable as it reflects actual model performance impact, less biased than MDI.\n",
        "\n",
        "Limitation:\n",
        "More computationally intensive than MDI.\n",
        "\n",
        "**Q:6-Explain the working principle of a Bagging Classifier.**\n",
        "**ANS:**\n",
        "A Bagging Classifier (Bootstrap Aggregating) is an ensemble learning method that improves model stability and accuracy by reducing variance and preventing overfitting. It works by combining predictions from multiple independent base models, typically high-variance models like decision trees, trained on different random subsets of the training data.\n",
        "\n",
        "**Working Principle of a Bagging Classifier:-**\n",
        "-The process involves three main steps: bootstrapping, parallel training, and aggregation.\n",
        "\n",
        "1)Bootstrap Sampling (Random Sampling with Replacement):-\n",
        "\n",
        "-Multiple new training subsets are created from the original dataset.\n",
        "\n",
        "-Each subset is generated by randomly selecting data points with replacement, meaning a single data point can appear multiple times in one subset, while other data points might not appear at all. This process ensures diversity among the training sets.\n",
        "\n",
        "2)Parallel Training:\n",
        "\n",
        "-A separate, independent base classifier (e.g., a decision tree, SVM, or neural network) is trained on each of the unique bootstrap samples in parallel.\n",
        "\n",
        "-These base models learn different patterns from their slightly varied data subsets.\n",
        "\n",
        "3)Aggregation:\n",
        "\n",
        "-Once all the base models are trained, they each make a prediction for new, unseen data.\n",
        "\n",
        "-For classification tasks, the final prediction is determined by a majority vote (hard voting) among all the individual model predictions. The class chosen by the most models becomes the ensemble's final classification. (For regression tasks, the predictions would be averaged).\n",
        "\n",
        "#Key Benefits:-\n",
        "\n",
        "1)Reduces Overfitting and Variance: By training on varied subsets and combining results, the impact of noise or outliers in any single dataset is minimized, leading to a more robust model that generalizes better to new data.\n",
        "\n",
        "2)Improved Accuracy and Stability: The combined ensemble model typically achieves higher accuracy and stability than any single base model would on its own.\n",
        "\n",
        "3)Flexibility: Bagging can be applied to a wide range of base learning algorithms.\n",
        "\n",
        "**Q:7-How do you evaluate a Bagging Classifier's performance?**\n",
        "**ANS:**\n",
        "To evaluate a Bagging Classifier, use standard classification metrics like Accuracy, Precision, Recall, and F1-Score, focusing on Confusion Matrix for detailed class performance and ROC/AUC for probabilistic evaluation, alongside using Out-of-Bag (OOB) estimates for unbiased validation and tuning hyperparameters like the number of estimators and base learner settings.\n",
        "\n",
        "#Key Evaluation Metrics:-\n",
        "\n",
        "1)Accuracy: Overall correct predictions (good for balanced datasets).\n",
        "\n",
        "2)Precision: How many selected items are relevant (True Positives / (True Positives + False Positives)).\n",
        "\n",
        "3)Recall (Sensitivity): How many relevant items are selected (True Positives / (True Positives + False Negatives)).\n",
        "\n",
        "4)F1-Score: Harmonic mean of Precision and Recall, great for imbalanced data.\n",
        "\n",
        "5)Confusion Matrix: Visualizes True Positives, False Positives, True Negatives, and False Negatives for detailed class insights.\n",
        "\n",
        "6)ROC Curve & AUC: Measures model's ability to distinguish classes across different thresholds.\n",
        "\n",
        "#Key Techniques:-\n",
        "\n",
        "1)Out-of-Bag (OOB) Score:-\n",
        "-Bagging naturally provides an internal validation set (OOB samples) for each tree, giving an unbiased performance estimate without needing a separate validation set.\n",
        "\n",
        "2)Test Set Evaluation:-\n",
        "-Use a completely separate test dataset (after training) to get final, unbiased performance metrics like accuracy, precision, etc..\n",
        "\n",
        "3)Hyperparameter Tuning:-\n",
        "-Tune parameters like n_estimators (number of base models) and base model hyperparameters (e.g., tree depth) using cross-validation or OOB scores to find optimal settings.\n",
        "\n",
        "#Steps to Evaluate:-\n",
        "\n",
        "1)Split Data: Divide data into training and test sets.\n",
        "\n",
        "2)Train Model: Train BaggingClassifier on the training data.\n",
        "\n",
        "3)Predict: Use the trained model to predict on the test set.\n",
        "\n",
        "4)Calculate Metrics: Compute Accuracy, F1, Precision, Recall, Confusion Matrix, and ROC/AUC on test predictions.\n",
        "\n",
        "5)Tune: Experiment with n_estimators and base model parameters, often using the OOB score for faster tuning.\n",
        "\n",
        "**Q:8-How does a Bagging Regressor work?**\n",
        "**ANS:**\n",
        "A Bagging Regressor (Bootstrap Aggregating) is an ensemble machine learning algorithm that improves model stability and accuracy by combining predictions from multiple independent base regression models. It primarily works to reduce variance and minimize overfitting in high-variance base models, such as decision trees.\n",
        "\n",
        "#How It Works\n",
        "-The process involves three main steps:\n",
        "\n",
        "1)Bootstrap Sampling:-\n",
        "-Multiple random subsets of the original training dataset are created by sampling with replacement. This means some data points may appear multiple times in a given subset, while others may be left out entirely. Each subset typically has the same number of instances as the original dataset.\n",
        "\n",
        "2)Parallel Training:-\n",
        "-A separate base regression model (often a decision tree regressor by default, as in scikit-learn's BaggingRegressor) is trained independently and in parallel on each of these unique bootstrap samples. This diversity in training data ensures the individual models learn slightly different patterns.\n",
        "\n",
        "3)Aggregation of Predictions:-\n",
        "-Once all base models are trained, the Bagging Regressor combines their outputs to make a final, more robust prediction. For regression tasks, the final prediction is the average (mean) of all the individual predictions from the base models.\n",
        "\n",
        "#Key Benefits:-\n",
        "\n",
        "1)Reduces Variance:-\n",
        "-By averaging the predictions of multiple diverse models, the ensemble's overall variance is significantly lower than that of any single model.\n",
        "\n",
        "2)Mitigates Overfitting:-\n",
        "-Training on different subsets of data helps the ensemble generalize better to unseen data, reducing the risk of overfitting.\n",
        "\n",
        "3)Increases Stability and Accuracy:-\n",
        "-The combined result is more stable and reliable, as the impact of noise or outliers in any single training set is averaged out across the ensemble.\n",
        "\n",
        "4)Parallelizable:-\n",
        "-Since each model is trained independently, the entire process can be run in parallel, which is computationally efficient for large datasets and complex problems.\n",
        "\n",
        "#Example:-\n",
        "\n",
        "-The most prominent example of a bagging regressor is the Random Forest Regressor, which extends bagging by also introducing random feature selection during the training of each decision tree to further reduce correlation among the trees.\n",
        "\n",
        "**Q:9-What is the main advantage of ensemble techniques?**\n",
        "**ANS:**\n",
        "The main advantage of ensemble techniques is their ability to significantly improve predictive accuracy and robustness by combining multiple models, which reduces errors, overfitting, and variance compared to any single model, leading to more stable and reliable predictions. They achieve this by leveraging diverse models to cancel out individual mistakes and capture complex patterns, making them highly effective for complex problems.\n",
        "\n",
        "#Key Benefits Explained:-\n",
        "\n",
        "1)Higher Accuracy:-\n",
        "-Aggregating predictions from several models often yields better results than the best individual model.\n",
        "\n",
        "2)Reduced Overfitting:-\n",
        "-Methods like bagging (Random Forests) train on data subsets, averaging out errors and preventing reliance on noise.\n",
        "\n",
        "3)Increased Robustness:-\n",
        "-Ensembles are less sensitive to outliers or noisy data because other models can compensate for errors made by one.\n",
        "\n",
        "4)Lower Variance & Bias:-\n",
        "-Averaging reduces variance, while techniques like boosting focus on reducing bias, balancing the model.\n",
        "\n",
        "5)Better Generalization:-\n",
        "-By considering different models or data subsets, ensembles capture a wider range of patterns, performing better on unseen data.\n",
        "\n",
        "#How it Works (Examples):\n",
        "\n",
        "1)Bagging (Random Forest):-\n",
        "-Trains multiple trees on different data samples and votes on the outcome, reducing variance.\n",
        "\n",
        "2)Boosting (Gradient Boosting):-\n",
        "-Builds models sequentially, with each new model correcting the previous one's errors, reducing bias.\n",
        "\n",
        "3)Stacking:-\n",
        "-Uses a meta-model to learn how to best combine predictions from various base models.\n",
        "\n",
        "**Q:10-What is the main challenge of ensemble methods?**\n",
        "**ANS:**\n",
        "The main challenges of ensemble methods are increased computational cost & time, reduced interpretability (black box nature), and the need for diversity among base models for effective error correction, alongside potential overfitting if not tuned correctly.\n",
        "\n",
        "#Key Challenges:-\n",
        "\n",
        "1)Computational Complexity:-\n",
        "-Training multiple models (especially with boosting) requires significant processing power and memory, increasing training time and resource usage.\n",
        "\n",
        "2)Interpretability:-\n",
        "-Complex ensembles (like Stacking) become \"black boxes,\" making it hard to understand why a specific prediction was made, which is a drawback in fields like healthcare.\n",
        "\n",
        "3)Model Diversity:-\n",
        "-The ensemble's success hinges on base models making different types of errors; similar, weak models won't improve performance much.\n",
        "\n",
        "4)Overfitting Risk:-\n",
        "-While they reduce overfitting, overly complex ensembles or poor tuning can still lead to overfitting the training data.\n",
        "\n",
        "5)Implementation & Tuning:-\n",
        "-Selecting the right base models, hyperparameters, and combination strategy can be complex and daunting.\n",
        "\n",
        "**Q:11-Explain the key idea behind ensemble techniques**.\n",
        "**ANS:**\n",
        "The key idea behind ensemble techniques is combining multiple individual machine learning models (often \"weak learners\") to create a single, stronger model with better predictive performance, accuracy, and robustness than any single model could achieve alone, essentially leveraging the wisdom of the crowd to average out errors and compensate for individual model weaknesses. This \"teamwork\" approach, like asking several experts for advice, creates a more reliable \"strong learner\" by aggregating diverse perspectives and reducing overfitting.\n",
        "\n",
        "#Core Principles:-\n",
        "\n",
        "1)Wisdom of the Crowd:-\n",
        "-A group decision is often better than a single expert's opinion, as individual mistakes get canceled out.\n",
        "\n",
        "2)Diversity:-\n",
        "-Using models that make different errors helps create a more balanced final prediction.\n",
        "\n",
        "3)Combining Weak Learners:-\n",
        "-Even simple models (weak learners) that barely perform better than random guessing can, when combined, form a powerful \"strong learner\".\n",
        "\n",
        "**Q:12-What is a Random Forest Classifier?**\n",
        "**ANS:**\n",
        "A Random Forest Classifier is a powerful ensemble machine learning model that combines many individual decision trees to make accurate classification predictions, using a \"majority vote\" system where the most predicted class wins, effectively reducing overfitting and handling complex data by averaging the outputs of multiple models. It works by training trees on random subsets of data and features, creating a diverse \"forest\" where each tree votes for a class, leading to a more robust final decision than a single tree.\n",
        "\n",
        "#How it works:\n",
        "1)Ensemble Learning:-\n",
        "-It's an ensemble method, meaning it combines multiple simpler models (decision trees) to improve overall performance.\n",
        "\n",
        "2)Bagging (Bootstrap Aggregating):-\n",
        "-It creates multiple training datasets by sampling the original data with replacement (bootstrapping).\n",
        "\n",
        "3)Random Feature Selection:-\n",
        "-At each split in a tree, only a random subset of features is considered, further increasing diversity.\n",
        "\n",
        "4)Voting:-\n",
        "-Each tree makes a prediction, and the final output is the class that receives the most votes from all the trees in the forest.\n",
        "\n",
        "#Key Advantages:\n",
        "\n",
        "1)High Accuracy:-\n",
        "-Often more accurate and stable than single decision trees.\n",
        "Reduces Overfitting: By averaging many trees, it avoids fitting noise in the data.\n",
        "\n",
        "2)Handles High Dimensions:-\n",
        "-Well-suited for datasets with many features.\n",
        "\n",
        "3)Versatile:-\n",
        "-Can be used for both classification (predicting categories) and regression (predicting values).\n",
        "\n",
        "**Q:13-What are the main types of ensemble techniques?**\n",
        "**ANS:**\n",
        "Ensemble Learning in machine learning that integrates multiple models called as weak learners to create a single effective model for prediction. This technique is used to enhance accuracy, minimizing variance and removing overfitting.\n",
        "\n",
        "#The main types of ensemble techniques are:\n",
        "\n",
        "1)Bagging (Bootstrap Aggregating):-\n",
        "\n",
        "-How it works: Trains multiple base models in parallel on different random subsets (with replacement) of the training data (bootstrapping) and combines their predictions (e.g., by averaging or majority vote).\n",
        "\n",
        "-Goal: Reduces variance and prevents overfitting.\n",
        "\n",
        "-Example: Random Forest (which uses decision trees).\n",
        "\n",
        "2)Boosting:-\n",
        "\n",
        "-How it works: Trains models sequentially, with each new model focusing on correcting the errors of the previous one, often by assigning higher weights to misclassified samples.\n",
        "\n",
        "-Goal: Reduces bias and improves overall accuracy.\n",
        "\n",
        "-Examples: AdaBoost, Gradient Boosting Machines (GBM), XGBoost, LightGBM.\n",
        "\n",
        "3)Stacking (Stacked Generalization):-\n",
        "\n",
        "-How it works: Trains different types of base models (Level 0) and then uses a meta-model (Level 1) to learn how to best combine their predictions for the final output.\n",
        "\n",
        "-Goal: Leverages the strengths of diverse models to create a more robust predictor.\n",
        "\n",
        "-Variation: Blending uses a holdout set to train the meta-model, preventing leakage.\n",
        "\n",
        "**Q:14-What is ensemble learning in machine learning?**\n",
        "**ANS:**\n",
        "Ensemble learning is a machine learning technique that combines multiple individual models (base learners) to produce a single, more accurate, and robust predictive model, leveraging the \"wisdom of the crowd\" to outperform any single model. It works by aggregating predictions from various models, which helps reduce errors and biases, making it great for complex tasks like image recognition, finance, and medical diagnosis. Common methods include Bagging (like Random Forests) and Boosting, which train models differently but aim for collective strength.\n",
        "\n",
        "#How it works:-\n",
        "\n",
        "1)Teamwork:-\n",
        "-Instead of relying on one potentially flawed model, ensemble methods use many, where each model offers a different perspective, like a group project where diverse ideas improve the outcome.\n",
        "\n",
        "2)Aggregation:-\n",
        "-The final prediction comes from combining the outputs of all individual models, often through averaging (for regression) or voting (for classification).\n",
        "\n",
        "3)Error Correction:-\n",
        "-Individual models might make mistakes, but their errors tend to cancel each other out in the final aggregated result, leading to higher overall accuracy and reliability.\n",
        "\n",
        "#Key techniques:-\n",
        "\n",
        "1)Bagging (Bootstrap Aggregating):-\n",
        "-Trains multiple models independently on different random subsets of the data, then averages their predictions (e.g., Random Forest).\n",
        "\n",
        "2)Boosting:-\n",
        "-Builds models sequentially, with each new model focusing on correcting errors made by the previous ones (e.g., AdaBoost, Gradient Boosting).\n",
        "\n",
        "3)Stacking (Stacked Generalization):-\n",
        "-Trains a \"meta-model\" to learn how to best combine the predictions from several base models.\n",
        "\n",
        "**Q:15-When should we avoid using ensemble methods?**\n",
        "**ANS:**\n",
        "Avoid ensemble methods when computational resources are limited, interpretability is crucial, data is small or models are highly correlated, or when a simple, fast individual model suffices, as ensembles add complexity, training time, and potential for overfitting without enough diverse data or if base models are too similar.\n",
        "\n",
        "#Reasons to Avoid Ensembles:-\n",
        "\n",
        "1)Limited Computational Power:-\n",
        "-Training multiple models (base learners) requires significant time, memory, and processing power, which can be prohibitive in resource-constrained environments.\n",
        "\n",
        "2)Need for Interpretability:-\n",
        "-Ensembles combine many models, making the final decision process opaque and difficult to explain, unlike a single decision tree or linear model.\n",
        "\n",
        "3)Small Datasets:-\n",
        "-If you don't have enough diverse data, an ensemble might not find enough variation among base models, or it could easily overfit the limited data.\n",
        "\n",
        "4)Highly Correlated Models:-\n",
        "-If your base models are too similar (e.g., all focusing on the same few features), their combined errors won't cancel out, offering little benefit over a single model.\n",
        "\n",
        "5)Time Constraints:-\n",
        "-Longer training and prediction times are inherent to ensembles, making them less suitable for real-time or rapid deployment scenarios.\n",
        "\n",
        "6)Simple Problems:-\n",
        "-For straightforward problems where a single strong model already performs well, the added complexity of an ensemble isn't justified.\n",
        "\n",
        "7)Specific Probabilistic Tasks:-\n",
        "-Some ensemble methods, like boosting, might produce poor probability estimates, making them less ideal for tasks where precise probabilities are critical.\n",
        "\n",
        "**Q:16-How does Bagging help in reducing overfitting?**\n",
        "**ANS:**\n",
        "Bagging (Bootstrap Aggregating) reduces overfitting by training multiple models on different random subsets (bootstrap samples) of the data and averaging their predictions, which significantly lowers the overall model's variance, making it less sensitive to noise and better at generalizing to new data. It works by combining diverse \"expert\" opinions from individual models, effectively canceling out individual model errors and producing a more stable, reliable, and robust final prediction.\n",
        "\n",
        "#How Bagging Reduces Variance and Overfitting:-\n",
        "\n",
        "1)Bootstrap Sampling:-\n",
        "-Instead of training on the full dataset, bagging creates multiple smaller datasets by randomly sampling from the original data with replacement.\n",
        "\n",
        "2)Parallel Training:-\n",
        "-A separate base model (like a decision tree) is trained on each of these unique bootstrap samples.\n",
        "\n",
        "3)Aggregation:-\n",
        "\n",
        "a)For classification, the final prediction is determined by a majority vote of all models.\n",
        "\n",
        "b)For regression, the final prediction is the average of all individual model predictions.\n",
        "\n",
        "**Q:17-Why is Random Forest better than a single Decision Tree?**\n",
        "**ANS:**\n",
        "Random Forest is generally better than a single Decision Tree because it significantly reduces overfitting and improves accuracy by averaging predictions from many trees, making it more robust, stable, and better at handling complex, high-dimensional data, even though it sacrifices some interpretability and is computationally heavier. A single tree easily learns noise and performs poorly on new data, whereas the ensemble approach of Random Forest leverages \"wisdom of the crowd\" for better generalization.\n",
        "\n",
        "#Key Advantages of Random Forest:-\n",
        "\n",
        "1)Reduces Overfitting:-\n",
        "-By combining many trees trained on random subsets of data and features, it averages out errors, preventing the model from memorizing noise in the training data.\n",
        "\n",
        "2)Higher Accuracy & Stability:-\n",
        "-The collective decision (majority vote or average) of multiple trees leads to more accurate and stable predictions than any single tree.\n",
        "\n",
        "3)Handles Complex Data:-\n",
        "-It performs well with large datasets, high-dimensional features, and can manage missing data effectively without pre-processing like normalization.\n",
        "\n",
        "4)Robustness:-\n",
        "-Less sensitive to outliers and noisy data points compared to a single tree.\n",
        "\n",
        "**Q:18-What is the role of bootstrap sampling in Bagging?**\n",
        "**ANS:**\n",
        "Bootstrap sampling is crucial in Bagging (Bootstrap Aggregation) as it creates multiple diverse training datasets by sampling with replacement from the original data, allowing independent training of base models, which, when aggregated (averaged/voted), significantly reduces variance, prevents overfitting, and improves the overall stability and accuracy of the ensemble model.\n",
        "\n",
        "#How it works:-\n",
        "\n",
        "1)Sampling:-\n",
        "-Create 'N' new datasets (bootstrap samples) from the original training data by randomly picking data points with replacement. This means some original data points might appear multiple times in a sample, while others are left out.\n",
        "\n",
        "2)Independent Training:-\n",
        "-Train a separate, often simple, base model (like a decision tree) on each unique bootstrap sample.\n",
        "\n",
        "3)Aggregation:-\n",
        "-Combine the predictions from all these individual models:\n",
        "\n",
        "a)Classification: Use majority voting (hard voting).\n",
        "\n",
        "b)Regression: Average the predictions (soft voting).\n",
        "\n",
        "**Q:19-What are some real-world applications of ensemble techniques?**\n",
        "**ANS:**\n",
        "Ensemble techniques combine multiple models for superior accuracy, widely used in finance (fraud detection, credit scoring), healthcare (disease diagnosis, patient risk), e-commerce (recommendations, churn prediction), cybersecurity (intrusion/malware detection), and autonomous vehicles (object detection, navigation), leveraging methods like Random Forests and Boosting for robust predictions in complex scenarios.\n",
        "\n",
        "#Finance:-\n",
        "\n",
        "1)Fraud Detection: Identifying unusual transaction patterns to catch fraudulent activities.\n",
        "\n",
        "2)Credit Scoring: Predicting loan default risk by combining various financial indicators.\n",
        "\n",
        "3)Stock Market Prediction: Forecasting market trends and risks by aggregating multiple model outputs.\n",
        "\n",
        "#Healthcare:-\n",
        "\n",
        "1)Disease Prediction: Diagnosing illnesses (like diabetes, heart disease) more accurately from medical images or patient data.\n",
        "\n",
        "2)Patient Readmission: Identifying high-risk patients to reduce hospital readmissions.\n",
        "\n",
        "3)Treatment Outcomes: Predicting effective treatments by combining different model insights.\n",
        "\n",
        "#E-commerce & Retail:-\n",
        "\n",
        "1)Recommendation Systems: Suggesting products by analyzing user behavior and past purchases.\n",
        "\n",
        "2)Customer Churn Prediction: Retaining customers by predicting who might leave.\n",
        "\n",
        "#Cybersecurity:-\n",
        "\n",
        "1)Intrusion Detection: Detecting network attacks and anomalous system behavior.\n",
        "\n",
        "2)Malware Detection: Combining models to accurately classify malicious software.\n",
        "\n",
        "#Autonomous Vehicles & Robotics:-\n",
        "\n",
        "1)Object/Lane Detection: Processing sensor data (camera, LiDAR) for safe navigation.\n",
        "\n",
        "2)Obstacle Avoidance: Integrating predictions for reliable pathfinding.\n",
        "\n",
        "**Q:20-What is the difference between Bagging and Boosting?**\n",
        "**ANS:**\n",
        "Bagging (Bootstrap Aggregating) trains parallel, independent models on data subsets to reduce variance (overfitting) by averaging predictions, while Boosting trains models sequentially, with each new model focusing on errors from the last to reduce bias (underfitting), making it more powerful but sensitive to outliers. Key differences: Bagging uses equal weights and parallel training; Boosting uses weighted samples (more for errors) and sequential training.\n",
        "\n",
        "#Bagging (e.g., Random Forest):-\n",
        "\n",
        "1)Goal: Reduce variance, improve stability, prevent overfitting.\n",
        "\n",
        "2)Method: Creates multiple training subsets via bootstrap sampling (with replacement) and trains models in parallel.\n",
        "\n",
        "3)Data Handling: Each model gets equal weight; models are independent.\n",
        "\n",
        "4)Performance: Robust to outliers due to averaging, faster to train (parallel).\n",
        "\n",
        "#Boosting (e.g., AdaBoost, XGBoost):-\n",
        "\n",
        "1)Goal: Reduce bias, improve accuracy by focusing on difficult examples.\n",
        "\n",
        "2)Method: Trains models sequentially, with each model learning from the errors (misclassified points) of the previous one.\n",
        "\n",
        "3)Data Handling: Samples are weighted; later models focus more on misclassified points.\n",
        "\n",
        "4)Performance: More powerful but less robust to outliers; slower to train (sequential).\n",
        "\n",
        "#Analogy\n",
        "\n",
        "1)Bagging: A team of chefs (models) independently cooking the same dish using slightly different recipe variations (data subsets) and then averaging their results.\n",
        "\n",
        "2)Boosting: A relay race where each runner (model) improves on the last runner's performance, with a focus on correcting mistakes, leading to a stronger final result."
      ],
      "metadata": {
        "id": "bqCH3vfA01P6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q:21-Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "base_model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "bagging_model = BaggingClassifier(\n",
        "    estimator=base_model,\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = bagging_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Bagging Classifier Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cC5sWHelAvYx",
        "outputId": "5df5894c-77fd-407f-9787-7c743134975b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q:22-Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE).\n",
        "\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X, y = make_regression(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    noise=15,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "base_model = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "bagging_regressor = BaggingRegressor(\n",
        "    estimator=base_model,\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "y_pred = bagging_regressor.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHuHFGX7jJhG",
        "outputId": "ab40f3f9-bc8a-4f97-a942-a418834822d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 2858.067084500636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q:23-Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "importances = rf_model.feature_importances_\n",
        "\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    \"Feature\": feature_names,\n",
        "    \"Importance\": importances\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "print(feature_importance_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4XMXF21jnFi",
        "outputId": "34530f03-d158-4437-9621-cf8577f1871c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    Feature  Importance\n",
            "23               worst area    0.153892\n",
            "27     worst concave points    0.144663\n",
            "7       mean concave points    0.106210\n",
            "20             worst radius    0.077987\n",
            "6            mean concavity    0.068001\n",
            "22          worst perimeter    0.067115\n",
            "2            mean perimeter    0.053270\n",
            "0               mean radius    0.048703\n",
            "3                 mean area    0.047555\n",
            "26          worst concavity    0.031802\n",
            "13               area error    0.022407\n",
            "21            worst texture    0.021749\n",
            "25        worst compactness    0.020266\n",
            "10             radius error    0.020139\n",
            "5          mean compactness    0.013944\n",
            "1              mean texture    0.013591\n",
            "12          perimeter error    0.011303\n",
            "24         worst smoothness    0.010644\n",
            "28           worst symmetry    0.010120\n",
            "16          concavity error    0.009386\n",
            "4           mean smoothness    0.007285\n",
            "19  fractal dimension error    0.005321\n",
            "15        compactness error    0.005253\n",
            "29  worst fractal dimension    0.005210\n",
            "11            texture error    0.004724\n",
            "14         smoothness error    0.004271\n",
            "18           symmetry error    0.004018\n",
            "9    mean fractal dimension    0.003886\n",
            "8             mean symmetry    0.003770\n",
            "17     concave points error    0.003513\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q:24-Train a Random Forest Regressor and compare its performance with a single Decision Tree.\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "dt_model = DecisionTreeRegressor(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "dt_pred = dt_model.predict(X_test)\n",
        "\n",
        "dt_mse = mean_squared_error(y_test, dt_pred)\n",
        "dt_r2 = r2_score(y_test, dt_pred)\n",
        "\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "rf_pred = rf_model.predict(X_test)\n",
        "\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "rf_r2 = r2_score(y_test, rf_pred)\n",
        "\n",
        "print(\"Decision Tree Regressor:\")\n",
        "print(\"MSE:\", dt_mse)\n",
        "print(\"R² Score:\", dt_r2)\n",
        "\n",
        "print(\"\\nRandom Forest Regressor:\")\n",
        "print(\"MSE:\", rf_mse)\n",
        "print(\"R² Score:\", rf_r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daTR-SaykJUY",
        "outputId": "ada0ad67-2b8e-44c9-e3a3-1dd534d8bf23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Regressor:\n",
            "MSE: 0.495235205629094\n",
            "R² Score: 0.622075845135081\n",
            "\n",
            "Random Forest Regressor:\n",
            "MSE: 0.2553684927247781\n",
            "R² Score: 0.8051230593157366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q:25-Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    oob_score=True,\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Out-of-Bag (OOB) Score:\", rf_model.oob_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOh7GHUskg1K",
        "outputId": "d2598e1e-4a01-4cc8-b2f6-089afdd0dbc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Out-of-Bag (OOB) Score: 0.9604395604395605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q:26-Train a Bagging Classifier using SVM as a base estimator and print accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "svm_base = SVC(\n",
        "    kernel='rbf',\n",
        "    probability=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bagging_clf = BaggingClassifier(\n",
        "    estimator=svm_base,\n",
        "    n_estimators=10,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Bagging Classifier with SVM Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jClh7V4l96C",
        "outputId": "88a1635e-797d-40e3-d325-44cf42f61549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier with SVM Accuracy: 0.9473684210526315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q:27-Train a Random Forest Classifier with different numbers of trees and compare accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "n_estimators_list = [10, 50, 100, 200]\n",
        "\n",
        "accuracy_results = {}\n",
        "\n",
        "for n in n_estimators_list:\n",
        "    rf_model = RandomForestClassifier(\n",
        "        n_estimators=n,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    y_pred = rf_model.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_results[n] = accuracy\n",
        "\n",
        "print(\"Random Forest Accuracy for Different Numbers of Trees:\\n\")\n",
        "for n,acc in accuracy_results.items():\n",
        "    print(f\"n_estimators = {n}: Accuracy = {acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUzWEDQlmXOD",
        "outputId": "107ea893-84b3-43f9-cfac-16bed8322f41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy for Different Numbers of Trees:\n",
            "\n",
            "n_estimators = 10: Accuracy = 0.9561\n",
            "n_estimators = 50: Accuracy = 0.9649\n",
            "n_estimators = 100: Accuracy = 0.9649\n",
            "n_estimators = 200: Accuracy = 0.9649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q:28-Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "base_model = LogisticRegression(max_iter=1000, solver='liblinear')\n",
        "\n",
        "bagging_clf = BaggingClassifier(\n",
        "    estimator=base_model,\n",
        "    n_estimators=50,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "y_prob = bagging_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "print(\"ROC-AUC Score:\", auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haznmQKFl5cu",
        "outputId": "c05813b6-3d53-44bc-d51b-8170112a4eec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9897780373831776\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q:29-Train a Random Forest Regressor and analyze feature importance scores.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
        "print(\"R² Score:\", r2_score(y_test, y_pred))\n",
        "\n",
        "feature_importance = pd.DataFrame({\n",
        "    \"Feature\": X.columns,\n",
        "    \"Importance\": rf.feature_importances_\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importance Scores:\")\n",
        "print(feature_importance)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUGtCFSMmbWx",
        "outputId": "4dfe3318-a829-4f2c-c8ef-3148110c5709"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.25650512920799395\n",
            "R² Score: 0.8045734925119942\n",
            "\n",
            "Feature Importance Scores:\n",
            "      Feature  Importance\n",
            "0      MedInc    0.526011\n",
            "5    AveOccup    0.138220\n",
            "7   Longitude    0.086124\n",
            "6    Latitude    0.086086\n",
            "1    HouseAge    0.054654\n",
            "2    AveRooms    0.047188\n",
            "4  Population    0.031722\n",
            "3   AveBedrms    0.029995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q:30-Train an ensemble model using both Bagging and Random Forest and compare accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "bagging.fit(X_train, y_train)\n",
        "bagging_pred = bagging.predict(X_test)\n",
        "bagging_acc = accuracy_score(y_test, bagging_pred)\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "rf_pred = rf.predict(X_test)\n",
        "rf_acc = accuracy_score(y_test, rf_pred)\n",
        "\n",
        "print(\"Bagging Classifier Accuracy:\", bagging_acc)\n",
        "print(\"Random Forest Accuracy:\", rf_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFQsBZwLmySj",
        "outputId": "5df55ac0-d5f4-47ba-c703-ac6afdd179be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Accuracy: 0.9415204678362573\n",
            "Random Forest Accuracy: 0.935672514619883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q:31-Train a Random Forest Classifier and tune hyperparameters using GridSearchCV.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "y_pred = best_rf.predict(X_test)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best CV Accuracy:\", grid_search.best_score_)\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "II4jQQyPnIYN",
        "outputId": "3b77ee39-872c-42f0-8e0b-e1e19aee7b68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Best CV Accuracy: 0.9725000000000001\n",
            "Test Accuracy: 0.935672514619883\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.91      0.91        64\n",
            "           1       0.94      0.95      0.95       107\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.93      0.93      0.93       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q:32-Train a Bagging Regressor with different numbers of base estimators and compare performance.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "n_estimators_list = [10, 50, 100, 200]\n",
        "\n",
        "results = []\n",
        "\n",
        "for n in n_estimators_list:\n",
        "    bagging = BaggingRegressor(\n",
        "        estimator=DecisionTreeRegressor(),\n",
        "        n_estimators=n,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    bagging.fit(X_train, y_train)\n",
        "    y_pred = bagging.predict(X_test)\n",
        "\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    results.append((n, mse, r2))\n",
        "\n",
        "results_df = pd.DataFrame(\n",
        "    results, columns=[\"n_estimators\", \"MSE\", \"R2\"]\n",
        ")\n",
        "\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoTT8416nh28",
        "outputId": "56b83798-0b59-4ad6-d3a6-6ff522c43880"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   n_estimators       MSE        R2\n",
            "0            10  0.286236  0.781922\n",
            "1            50  0.257874  0.803531\n",
            "2           100  0.256836  0.804321\n",
            "3           200  0.254165  0.806356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q:33-Train a Random Forest Classifier and analyze misclassified samples.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=None,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4u5godNiXh2u",
        "outputId": "0e1e0298-3a37-4f5c-e6a3-4a900572e856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.958041958041958\n",
            "\n",
            "Confusion Matrix:\n",
            " [[49  4]\n",
            " [ 2 88]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.92      0.94        53\n",
            "           1       0.96      0.98      0.97        90\n",
            "\n",
            "    accuracy                           0.96       143\n",
            "   macro avg       0.96      0.95      0.95       143\n",
            "weighted avg       0.96      0.96      0.96       143\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q:34-Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "\n",
        "dt_acc = accuracy_score(y_test, y_pred_dt)\n",
        "print(\"Decision Tree Accuracy:\", dt_acc)\n",
        "print(\"\\nDecision Tree Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred_dt))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaUfVbH4YNu1",
        "outputId": "4caa516c-d152-43da-cd46-4d6c68ea7845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 0.9230769230769231\n",
            "\n",
            "Decision Tree Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.92      0.90        53\n",
            "           1       0.95      0.92      0.94        90\n",
            "\n",
            "    accuracy                           0.92       143\n",
            "   macro avg       0.91      0.92      0.92       143\n",
            "weighted avg       0.92      0.92      0.92       143\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q:35-Train a Random Forest Classifier and visualize the confusion matrix.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(\n",
        "    cm,\n",
        "    annot=True,\n",
        "    fmt=\"d\",\n",
        "    cmap=\"Blues\",\n",
        "    xticklabels=data.target_names,\n",
        "    yticklabels=data.target_names\n",
        ")\n",
        "\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"Actual Label\")\n",
        "plt.title(\"Confusion Matrix - Random Forest Classifier\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "v99FrrR-Yzau",
        "outputId": "2cf121b7-7b48-4466-b9ad-7b27925c5842"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.958041958041958\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q:36-Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy.\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "\n",
        "svm = SVC(kernel='rbf', probability=True, random_state=42)\n",
        "svm.fit(X_train, y_train)\n",
        "svm_pred = svm.predict(X_test)\n",
        "\n",
        "lr = LogisticRegression(max_iter=5000)\n",
        "lr.fit(X_train, y_train)\n",
        "lr_pred = lr.predict(X_test)\n",
        "\n",
        "estimators = [\n",
        "    ('dt', DecisionTreeClassifier(random_state=42)),\n",
        "    ('svm', SVC(kernel='rbf', probability=True, random_state=42)),\n",
        "    ('lr', LogisticRegression(max_iter=5000))\n",
        "]\n",
        "\n",
        "stack_model = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LogisticRegression(),\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "stack_model.fit(X_train, y_train)\n",
        "stack_pred = stack_model.predict(X_test)\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, dt_pred))\n",
        "print(\"SVM Accuracy:\", accuracy_score(y_test, svm_pred))\n",
        "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, lr_pred))\n",
        "print(\"Stacking Classifier Accuracy:\", accuracy_score(y_test, stack_pred))\n"
      ],
      "metadata": {
        "id": "PzpaRoNGaP9U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08c19d19-fbd4-4ce9-b846-c0e526566863"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 0.951048951048951\n",
            "SVM Accuracy: 0.951048951048951\n",
            "Logistic Regression Accuracy: 0.965034965034965\n",
            "Stacking Classifier Accuracy: 0.965034965034965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q:37-Train a Random Forest Classifier and print the top 5 most important features.\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42\n",
        ")\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': rf.feature_importances_\n",
        "})\n",
        "\n",
        "top_5_features = feature_importance.sort_values(\n",
        "    by='Importance', ascending=False\n",
        ").head(5)\n",
        "\n",
        "print(top_5_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBOJOIp6ZNsT",
        "outputId": "659f0faf-dab6-4118-a598-7cdd594b6dad"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Feature  Importance\n",
            "27  worst concave points    0.172295\n",
            "23            worst area    0.123192\n",
            "7    mean concave points    0.090299\n",
            "6         mean concavity    0.083215\n",
            "20          worst radius    0.081277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q:38-Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "base_model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=base_model,\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bagging.fit(X_train, y_train)\n",
        "\n",
        "y_pred = bagging.predict(X_test)\n",
        "\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n",
        "\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sH-TrOOkZ6az",
        "outputId": "cce2daf6-1083-4a78-f7a5-1e5db788f2f2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.9662921348314607\n",
            "Recall: 0.9662921348314607\n",
            "F1-score: 0.9662921348314607\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.94      0.94        54\n",
            "           1       0.97      0.97      0.97        89\n",
            "\n",
            "    accuracy                           0.96       143\n",
            "   macro avg       0.96      0.96      0.96       143\n",
            "weighted avg       0.96      0.96      0.96       143\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q:39-Train a Random Forest Classifier and analyze the effect of max_depth on accuracy.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "depths = [1, 2, 3, 5, 7, 10, None]\n",
        "\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "for depth in depths:\n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=depth,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    rf.fit(X_train, y_train)\n",
        "\n",
        "    train_acc.append(accuracy_score(y_train, rf.predict(X_train)))\n",
        "    test_acc.append(accuracy_score(y_test, rf.predict(X_test)))\n",
        "\n",
        "for d, tr, te in zip(depths, train_acc, test_acc):\n",
        "    print(f\"max_depth={d}: Train Acc={tr:.3f}, Test Acc={te:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkbMC1-raY4v",
        "outputId": "70751c63-70f0-4e5b-ca01-4da4cae323d2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_depth=1: Train Acc=0.920, Test Acc=0.965\n",
            "max_depth=2: Train Acc=0.955, Test Acc=0.965\n",
            "max_depth=3: Train Acc=0.981, Test Acc=0.965\n",
            "max_depth=5: Train Acc=0.993, Test Acc=0.965\n",
            "max_depth=7: Train Acc=0.998, Test Acc=0.965\n",
            "max_depth=10: Train Acc=1.000, Test Acc=0.965\n",
            "max_depth=None: Train Acc=1.000, Test Acc=0.965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q:40-Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42\n",
        ")\n",
        "dt_base = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "bag_dt = BaggingRegressor(\n",
        "    estimator=dt_base,\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bag_dt.fit(X_train, y_train)\n",
        "dt_pred = bag_dt.predict(X_test)\n",
        "\n",
        "dt_mse = mean_squared_error(y_test, dt_pred)\n",
        "\n",
        "knn_base = KNeighborsRegressor(n_neighbors=5)\n",
        "\n",
        "bag_knn = BaggingRegressor(\n",
        "    estimator=knn_base,\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bag_knn.fit(X_train, y_train)\n",
        "knn_pred = bag_knn.predict(X_test)\n",
        "\n",
        "knn_mse = mean_squared_error(y_test, knn_pred)\n",
        "print(\"Bagging + Decision Tree MSE:\", dt_mse)\n",
        "print(\"Bagging + KNN MSE:\", knn_mse)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2mWeLQcbMSe",
        "outputId": "bb64ef1f-fd60-475d-ad73-8cef956e1653"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging + Decision Tree MSE: 0.2545352229141104\n",
            "Bagging + KNN MSE: 1.1005906642080172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q:41-Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_prob = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "print(\"ROC-AUC Score:\", roc_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RtoWQqPcDBM",
        "outputId": "8ac7adba-b779-4bc5-c0ef-3dca2de8b2f7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9962546816479401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q:42-Train a Bagging Classifier and evaluate its performance using cross-validation.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "base_model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "bagging_clf = BaggingClassifier(\n",
        "    estimator=base_model,\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "cv_scores = cross_val_score(\n",
        "    bagging_clf,\n",
        "    X,\n",
        "    y,\n",
        "    cv=5,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "print(\"Cross-Validation Accuracy Scores:\", cv_scores)\n",
        "print(\"Mean CV Accuracy:\", cv_scores.mean())\n",
        "print(\"Standard Deviation:\", cv_scores.std())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s25mocsdchRA",
        "outputId": "8887f586-54b1-4fa4-bc3a-b28542abf59f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Validation Accuracy Scores: [0.89473684 0.93859649 0.99122807 0.96491228 1.        ]\n",
            "Mean CV Accuracy: 0.9578947368421054\n",
            "Standard Deviation: 0.03819568606504778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q:43-Train a Random Forest Classifier and plot the Precision-Recall curve.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_scores = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "\n",
        "avg_precision = average_precision_score(y_test, y_scores)\n",
        "print(\"Average Precision (AP):\", avg_precision)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e5E2PwVc94G",
        "outputId": "bb4dc4f3-8794-4d4f-a9b2-c7bdf6c091f6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Precision (AP): 0.9975302353055252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q:44-Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_pred = rf.predict(X_test)\n",
        "\n",
        "lr = LogisticRegression(max_iter=5000)\n",
        "lr.fit(X_train, y_train)\n",
        "lr_pred = lr.predict(X_test)\n",
        "\n",
        "estimators = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('lr', LogisticRegression(max_iter=5000))\n",
        "]\n",
        "\n",
        "stack_clf = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LogisticRegression(),\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "stack_clf.fit(X_train, y_train)\n",
        "stack_pred = stack_clf.predict(X_test)\n",
        "\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf_pred))\n",
        "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, lr_pred))\n",
        "print(\"Stacking Classifier Accuracy:\", accuracy_score(y_test, stack_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHoa9OAOdUWx",
        "outputId": "b9febb5b-9419-4b3f-c3eb-00fb786e8515"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 0.965034965034965\n",
            "Logistic Regression Accuracy: 0.965034965034965\n",
            "Stacking Classifier Accuracy: 0.965034965034965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q:45-Train a Bagging Regressor with different levels of bootstrap samples and compare performance.\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "sample_sizes = [0.3, 0.5, 0.7, 1.0]\n",
        "results = {}\n",
        "\n",
        "for sample in sample_sizes:\n",
        "    bagging = BaggingRegressor(\n",
        "        estimator=DecisionTreeRegressor(random_state=42),\n",
        "        n_estimators=100,\n",
        "        max_samples=sample,\n",
        "        bootstrap=True,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    bagging.fit(X_train, y_train)\n",
        "    y_pred = bagging.predict(X_test)\n",
        "\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    results[sample] = mse\n",
        "\n",
        "for sample, mse in results.items():\n",
        "    print(f\"max_samples={sample}: MSE={mse:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrxbxwCxdshY",
        "outputId": "d0ec2a02-bdf6-4d62-fdc2-5adcb27d4965"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_samples=0.3: MSE=0.277\n",
            "max_samples=0.5: MSE=0.264\n",
            "max_samples=0.7: MSE=0.258\n",
            "max_samples=1.0: MSE=0.255\n"
          ]
        }
      ]
    }
  ]
}